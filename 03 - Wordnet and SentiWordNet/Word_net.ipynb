{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **What is WordNet?** ðŸ‘‡"
      ],
      "metadata": {
        "id": "l3TAKb2JfxAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNet is a lexical database that groups English words into sets of synonyms based on their meanings. These groups are called \"synsets\" and are linked together by semantic relationships such as antonyms, hypernyms, hyponyms, and meronyms. It is used in various applications of Natural Language Processing."
      ],
      "metadata": {
        "id": "9C4t1fchfZh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is how we incorporate WordNet into our python project."
      ],
      "metadata": {
        "id": "m_Cywmrkgc2f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CiBcRvW_ekHF",
        "outputId": "9e90d30d-2104-4ba3-8004-111753cbb500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Text: Inaugural Address Corpus>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import nltk\n",
        "import math\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk as lk\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('book')\n",
        "from nltk.book import *\n",
        "text4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Picking a Noun"
      ],
      "metadata": {
        "id": "I0T5bdGWgFys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code we will pick a noun and do various wordnet operations to take advantage of its features. First we get the synsets of a given noun"
      ],
      "metadata": {
        "id": "gr2-u96SgYn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synsets('enemy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dcDazv7DDoNH",
        "outputId": "839898e1-8585-42a2-e0c6-d1d89da5effb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('enemy.n.01'),\n",
              " Synset('enemy.n.02'),\n",
              " Synset('enemy.n.03'),\n",
              " Synset('foe.n.02')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sysnets are displayed now we will pick the first  one which is 'enemy.n.01' and we will output:\n",
        "\n",
        "*   Definition\n",
        "*   Usage example\n",
        "*   Lemma"
      ],
      "metadata": {
        "id": "_oNsIxR4F8RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset('enemy.n.01').definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FpwD7fGuGVLF",
        "outputId": "ab5edf4a-90b5-4886-eb2a-51983e3011b3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'an opposing military force'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset('enemy.n.01').examples()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XvKwNiTIHGsK",
        "outputId": "64b196e9-a206-4efc-8af9-3483832b177a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the enemy attacked at dawn']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset('enemy.n.01').lemmas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GHismE4lHdRN",
        "outputId": "ab86c97a-4c83-4426-b2dd-7b9046cebc67"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('enemy.n.01.enemy')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we iterate overall sysnets of the word enemy to extract all defintions, examples and lemmas"
      ],
      "metadata": {
        "id": "AveDBlE6HxSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enemy_synsets = wn.synsets('enemy', pos=wn.NOUN)\n",
        "for sense in enemy_synsets:\n",
        "    lemmas = [l.name() for l in sense.lemmas()]\n",
        "    print(\"Synset: \" + sense.name() + \"(\" +sense.definition() + \")  \\n\\t Lemmas:\" + str(lemmas))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2x2eZyQJH940",
        "outputId": "b0bcb6bf-a37a-4698-c215-5bf044c5ae2f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset: enemy.n.01(an opposing military force)  \n",
            "\t Lemmas:['enemy']\n",
            "Synset: enemy.n.02(an armed adversary (especially a member of an opposing military force))  \n",
            "\t Lemmas:['enemy', 'foe', 'foeman', 'opposition']\n",
            "Synset: enemy.n.03(any hostile group of people)  \n",
            "\t Lemmas:['enemy']\n",
            "Synset: foe.n.02(a personal enemy)  \n",
            "\t Lemmas:['foe', 'enemy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see WordNet has organized the nouns in from most alike definitions to least alike. It started with the military definition to a more personal definition of the word."
      ],
      "metadata": {
        "id": "IqtRtmmHI49s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we output the hypernyms, hypnoyms, meronyms, holonyms, and antonyms of our selected word. "
      ],
      "metadata": {
        "id": "E8hfdgMfOsSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enemy = wn.synset('enemy.n.01')\n",
        "print(enemy.hypernyms())\n",
        "print(enemy.hyponyms())\n",
        "print(enemy.part_meronyms())\n",
        "print(enemy.part_holonyms())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "97FmGEO5PIgq",
        "outputId": "41f7f410-b739-41b9-f574-952909436d94"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('military_unit.n.01')]\n",
            "[]\n",
            "[]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get an antonym it is a bit different but not difficult"
      ],
      "metadata": {
        "id": "LRhrGyX4fWku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enemy = wn.synsets('enemy', pos=wn.NOUN)[0]\n",
        "enemy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "824iomycetWa",
        "outputId": "8e1bc402-8cf6-44bf-b7d7-5021ad365d82"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('enemy.n.01')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enemy.lemmas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-eU9qqMkfqdt",
        "outputId": "692bde9a-c3c7-482e-b18b-a95d7429798e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('enemy.n.01.enemy')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enemy.lemmas()[0].antonyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pHzr0Z92fuwW",
        "outputId": "febda17c-d9dc-48c7-8835-ef880be7df8d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, our noun didn't have a antonym in wordnet therefore it returned null"
      ],
      "metadata": {
        "id": "jE8uAFDdgBIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Picking a verb"
      ],
      "metadata": {
        "id": "m7JdEBe9J4RA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will do the same as above but with the verb.\n",
        "\n"
      ],
      "metadata": {
        "id": "_D5eXOHFKIj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synsets('climb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nJa3nfr9Jacq",
        "outputId": "7a3e7e6d-0947-40ed-d76b-085aef8a41c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('ascent.n.01'),\n",
              " Synset('climb.n.02'),\n",
              " Synset('climb.n.03'),\n",
              " Synset('climb.v.01'),\n",
              " Synset('climb.v.02'),\n",
              " Synset('wax.v.02'),\n",
              " Synset('climb.v.04'),\n",
              " Synset('climb.v.05'),\n",
              " Synset('rise.v.02')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset('climb.v.01').definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HGHiZ3IdK-yj",
        "outputId": "403a6cba-a21b-460c-9fc0-956100818883"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go upward with gradual or continuous progress'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset('climb.v.01').examples()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1095y4sfLd7A",
        "outputId": "818a835d-5ec2-4526-f82a-3c42457b4975"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Did you ever climb up the hill behind your house?']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset('climb.v.01').lemmas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Qa-iTY-SLnB4",
        "outputId": "583fe3b8-06dc-42ef-d536-28852be90967"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('climb.v.01.climb'),\n",
              " Lemma('climb.v.01.climb_up'),\n",
              " Lemma('climb.v.01.mount'),\n",
              " Lemma('climb.v.01.go_up')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enemy_synsets = wn.synsets('climb', pos=wn.VERB)\n",
        "for sense in enemy_synsets:\n",
        "    lemmas = [l.name() for l in sense.lemmas()]\n",
        "    print(\"Synset: \" + sense.name() + \"(\" +sense.definition() + \")  \\n\\t Lemmas:\" + str(lemmas))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MMsFrzeENwPY",
        "outputId": "ea07c0a7-411c-4edc-d1c3-58bb2ae29d2b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset: climb.v.01(go upward with gradual or continuous progress)  \n",
            "\t Lemmas:['climb', 'climb_up', 'mount', 'go_up']\n",
            "Synset: climb.v.02(move with difficulty, by grasping)  \n",
            "\t Lemmas:['climb']\n",
            "Synset: wax.v.02(go up or advance)  \n",
            "\t Lemmas:['wax', 'mount', 'climb', 'rise']\n",
            "Synset: climb.v.04(slope upward)  \n",
            "\t Lemmas:['climb']\n",
            "Synset: climb.v.05(improve one's social status)  \n",
            "\t Lemmas:['climb']\n",
            "Synset: rise.v.02(increase in value or to a higher point)  \n",
            "\t Lemmas:['rise', 'go_up', 'climb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see above, we see the most common usuage at the top and as we go to the bottom of its uses we see more figurative/niche uses of the verb. "
      ],
      "metadata": {
        "id": "zT18iKV5N6jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Using Morphy"
      ],
      "metadata": {
        "id": "iZkvKU-iOXt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example we use morphy to find uses as many examples of a word as we can"
      ],
      "metadata": {
        "id": "ph7yEihLS-kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wn.morphy('steepest')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "l3am4NvbONl7",
        "outputId": "5827b5d7-6d33-44ab-dbc8-25aadc56eb8d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'steep'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.morphy('steeper')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bCY07GOLciDP",
        "outputId": "e09c39f4-174a-4850-c606-887bedd32831"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'steeper'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Using similarities"
      ],
      "metadata": {
        "id": "Iil9f3_EWQhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to pick two similar words and use the Wu-Palmer similarity metric and the Lesk algorithm to find some interesting information."
      ],
      "metadata": {
        "id": "LpnHSyYMWXMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Wu-Palmer's algorithm"
      ],
      "metadata": {
        "id": "H3b159JPYv28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_word = wn.synset('snake.n.01')\n",
        "second_word = wn.synset('python.n.01')\n",
        "print(first_word.definition())\n",
        "print(second_word.definition())\n",
        "wn.wup_similarity(first_word, second_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b1mc-OvFWmjt",
        "outputId": "8e10b2a0-27d8-4072-db53-99a86cdb0c40"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "limbless scaly elongate reptile; some are venomous\n",
            "large Old World boas\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8888888888888888"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using the Lesk algorithm"
      ],
      "metadata": {
        "id": "_vaa0haBY2cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lk('snake', 'python')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SfFqQVDcY43R",
        "outputId": "67a99572-9d4b-4655-c8fa-abeab9a6aca1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('python.n.02')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lk('python', 'snake')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CU7WvSgEcZ--",
        "outputId": "b0d76bed-f1fa-4133-9ea8-f35629a61862"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('snake.v.03')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset('python.n.02').definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "s8hmx3qYbCe-",
        "outputId": "112b13c1-bde6-4071-c551-0e89cad67d57"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a soothsaying spirit or a person who is possessed by such a spirit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset('snake.v.03').definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "I4OzJez3cf7_",
        "outputId": "c15fa56d-781e-4cf6-e446-b53167b5fe0d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'move along a winding path'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is interesting how both algorithms picked different words. It seems that Lesk picked the most colloquial version of the word. Lesk algorithm picked a more obscure match for the result. If I was to pick an algorithm I would go with Wu's algorithm since it gives me a value that closely relates each word and could be more applicable. The only time I would use the Lesk algorithm is that if I didn't need a specic definition"
      ],
      "metadata": {
        "id": "zMDQjAHZc00k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Sentiword"
      ],
      "metadata": {
        "id": "HjyQ_TT-eMzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentiWordNet is lexical took that is used for analyzing the senitment of a text. It groups words into three rankings: positivity, negativity, and objectivity. It can usd in various fields of NLP programs for example, it could be implemented to get the general opinion about a specific piece of media."
      ],
      "metadata": {
        "id": "9H1EkqadenXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The score is in a range from 0 - 1 where the closer the number is to 1 is the closer the feeling. "
      ],
      "metadata": {
        "id": "VLYzWpakiwG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for word_use in swn.senti_synsets('hate'):\n",
        "  print(\"\\nNegative score = \", word_use.neg_score())\n",
        "  print(\"Positive score = \", word_use.pos_score())\n",
        "  print(\"Objective score = \", word_use.obj_score())\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "I6p8yZeqjhhc",
        "outputId": "39dbad5a-f56c-4a60-d993-e9b4c40009cd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Negative score =  0.375\n",
            "Positive score =  0.125\n",
            "Objective score =  0.5\n",
            "\n",
            "Negative score =  0.75\n",
            "Positive score =  0.0\n",
            "Objective score =  0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word hate didn't always get a score of zero for postive. I assume this was the case because one may hate bad things, which is good. "
      ],
      "metadata": {
        "id": "L9VfR0-KjCoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will do the same thing but using a sentence"
      ],
      "metadata": {
        "id": "hYlU5EswkxYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Computer Science majors hate touching grass'\n",
        "\n",
        "neg = 0\n",
        "pos = 0\n",
        "tokens = sentence.split()\n",
        "for token in tokens:\n",
        "    print('\\nWord: ' + token)\n",
        "    syn_list = list(swn.senti_synsets(token))\n",
        "    if syn_list:\n",
        "        syn = syn_list[0]\n",
        "        neg += syn.neg_score()\n",
        "        pos += syn.pos_score()\n",
        "        print(syn.neg_score())\n",
        "        print(syn.pos_score())\n",
        "    \n",
        "print(\"\\nneg\\tpos counts\")\n",
        "print(neg, '\\t', pos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OjWj8rCulmXm",
        "outputId": "7a015636-7ba0-4951-b0fe-851bf75acce3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word: Computer\n",
            "0.0\n",
            "0.0\n",
            "\n",
            "Word: Science\n",
            "0.0\n",
            "0.0\n",
            "\n",
            "Word: majors\n",
            "0.0\n",
            "0.125\n",
            "\n",
            "Word: hate\n",
            "0.375\n",
            "0.125\n",
            "\n",
            "Word: touching\n",
            "0.0\n",
            "0.0\n",
            "\n",
            "Word: grass\n",
            "0.0\n",
            "0.0\n",
            "\n",
            "neg\tpos counts\n",
            "0.375 \t 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see with the sentence above the score is mostly negative since it is picking at CS majors and I would say SentiWordNet did a decent job. If the input text was larger then a better score can be given. Of course doing it like this has limitations since some words can be aplified with the context of surrounding words. For example, the word 'majors' got picked as postive even though in this context it doesn't shouldn't affect sentiment."
      ],
      "metadata": {
        "id": "wzBGwSxjoJEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Collocation"
      ],
      "metadata": {
        "id": "loyyFDDcpAve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Natural Language Processing, collocation refers to words in a langauge that often appear together. For instance, the words 'milk tea' often appear together due to being a popular beverage while the words 'milk soda' might not as much since they aren't a common combination (at least as far as I am aware.)"
      ],
      "metadata": {
        "id": "Q20yVWgSpHrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text4.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "z1iPk_8AaXV9",
        "outputId": "06aceb89-a356-4a06-c698-280e7281b624"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Inaugural Address Corpus'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4.collocations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lePTC6EVqJBz",
        "outputId": "e5b35fed-0014-497c-8e0a-29a823d6a200"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see there is common collocations in the text such as United States and American People which makes sense given the text4 is about the Inaugural Address Corpus"
      ],
      "metadata": {
        "id": "_tthkY8qXHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7: Mutual Infomration"
      ],
      "metadata": {
        "id": "aRdXp7ZObY4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mutual information is a statistic that tells us the association between two words in a given text. Where a higher Mutual Information indicates a higher dependence score."
      ],
      "metadata": {
        "id": "h4W_lcjWbftB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us take at a collocation from text4 and calcualte the mutual information on it"
      ],
      "metadata": {
        "id": "9MT8E3SQYHnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula is log2[P(x,y) / [P(x) * P(y)]] which matematically works as the log probabilities of each word appearing by itself and the probablities of each other appearing together. "
      ],
      "metadata": {
        "id": "Lon19986YwlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' '.join(text4.tokens)"
      ],
      "metadata": {
        "id": "EZGP9KoqXkIG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = len(set(text4))\n",
        "hg = text.count('United States')/vocab\n",
        "print(\"p(United States) = \",hg )\n",
        "h = text.count('United')/vocab\n",
        "print(\"p(United) = \", h)\n",
        "g = text.count('States')/vocab\n",
        "print('p(States) = ', g)\n",
        "pmi = math.log2(hg / (h * g))\n",
        "print('pmi = ', pmi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iPWWK3IFZmwz",
        "outputId": "d04a39dd-3059-4c48-c6f7-91bcac33e31d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p(United States) =  0.015860349127182045\n",
            "p(United) =  0.0170573566084788\n",
            "p(States) =  0.03301745635910224\n",
            "pmi =  4.815657649820885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = len(set(text4))\n",
        "hg = text.count('in the')/vocab\n",
        "print(\"p(in the) = \",hg )\n",
        "h = text.count('in ')/vocab\n",
        "print(\"p(in) = \", h)\n",
        "g = text.count('the ')/vocab\n",
        "print('p(the) = ', g)\n",
        "pmi = math.log2(hg / (h * g))\n",
        "print('pmi = ', pmi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eaRYSWqOa9Tk",
        "outputId": "bfbe4007-f4ee-4871-c448-c504405f9a6b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p(in the) =  0.09276807980049875\n",
            "p(in) =  0.30733167082294266\n",
            "p(the) =  0.9533167082294264\n",
            "pmi =  -1.659123548063425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see above, in the text given as input the words United States have a higher PMI than the words in the. Which means they have a higher dependency on each other. That being said if we were to use a different text then the scores could be different. This score can be useful in NLP since it could help us retrive information, analyze text, or get sentiment analysis."
      ],
      "metadata": {
        "id": "S0lgd-jQcoIt"
      }
    }
  ]
}